{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary modules\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter\n",
    "import dotenv\n",
    "import textwrap\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to print the response in wrapped format\n",
    "def print_response(response, width=70):\n",
    "    # wrap the response to fit within the specified width\n",
    "    wrapper = textwrap.TextWrapper(width=width) \n",
    "    # wrap the response\n",
    "    wrapped_string = wrapper.fill(response)\n",
    "    # print the wrapped response\n",
    "    print(wrapped_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OpenAI API key from secret file\n",
    "dotenv.load_dotenv()\n",
    "# language model to use\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0) # could also use Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'source': './documents_db/Matteucci_2024.pdf', 'page': 0},\n",
       " {'source': './documents_db/Matteucci_2024.pdf', 'page': 1},\n",
       " {'source': './documents_db/Matteucci_2024.pdf', 'page': 2}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pdf document\n",
    "loader = PyPDFLoader(\"./documents_db/Matteucci_2024.pdf\")\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 5000, chunk_overlap = 500) # important!\n",
    "paper_chunks = loader.load_and_split(text_splitter=splitter)\n",
    "# inspect the metadata of the loaded pages\n",
    "print(f\"Number of documents:\", len(paper_chunks))\n",
    "[paper_chunk.metadata for paper_chunk in paper_chunks[0:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the directory where the database should be located\n",
    "persist_directory = \"./chroma_db\"\n",
    "# check if the directory exists\n",
    "if os.path.exists(persist_directory):\n",
    "    # load persistent vectorstore from disk if it exists\n",
    "    vectorstore = Chroma(persist_directory=persist_directory, embedding_function=OpenAIEmbeddings())\n",
    "else:\n",
    "    # create persistent vectorstore if the directory does not exist\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        paper_chunks,\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "        persist_directory=persist_directory,       \n",
    "    )\n",
    "# define the retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10}) # could also use \"mmr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define contextualization subsystem prompt for history-aware retriever chain ---------\n",
    "contextualize_q_system_prompt = \"\"\"\n",
    "Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "# create history-aware retriever chain\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "# define the system prompt for the question-answering chain --------------------------\n",
    "qa_system_prompt = \"\"\"\n",
    "You are an assistant to Dr. Giulio Matteucci, a neuroscientist.\n",
    "Your role is to engage with visitors on his personal website, providing answers to their inquiries\n",
    " regarding Giulio's review paper you are given access to. Your responses should be scientifically precise,\n",
    " and accuratily reflect the content and message of the papers.\n",
    " It is of paramount importance not to make up any information, when in doubt,\n",
    "   just say that this question goes beyond the scope of the paper. \n",
    "   It is really important to ensure factual accuracy and avoid inventing concepts references and attributions.\n",
    "Strive to be clear and accessible for all users, be both professional and approachable,\n",
    " ensuring your explanations are succinct and schematic without sacrificing essential details.\n",
    "When answering questions, rely solely on the context provided by Dr. Matteucci's papers. \n",
    "Never mention anything that is not present in the papers.\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "# create question-answering chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "# create the final chain ----------------------------------------------------------\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset chat history\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Dr. Giulio Matteucci's review paper, he discusses the confluence\n",
      "between trends in neuroscience and machine learning, focusing on\n",
      "unsupervised learning. Specifically, the paper explores how sensory\n",
      "processing systems learn to exploit the statistical structure of their\n",
      "inputs without explicit training targets or rewards. The review\n",
      "highlights the influence of sensory experience on neural self-\n",
      "organization and synaptic bases, emphasizing the role of unsupervised\n",
      "learning in the development of neuronal tuning from middle- to high-\n",
      "order areas along cortical sensory processing hierarchies. The paper\n",
      "also discusses novel algorithms for unsupervised and self-supervised\n",
      "learning, their implications for theories of the brain, particularly\n",
      "in intermediate visual cortical areas, and their application in real-\n",
      "world learning machines.\n"
     ]
    }
   ],
   "source": [
    "# first conversational cycle\n",
    "question = \"What Giulio talks about in his review paper?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\n",
    "print_response(ai_msg_1[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In layman's terms, Unsupervised Temporal Learning (UTL) and\n",
      "Unsupervised Spatial Learning (USL) are ways in which our brains learn\n",
      "from the world around us without explicit instructions or rewards.  -\n",
      "UTL: This type of learning focuses on how our brains use the natural\n",
      "flow of time in visual experiences to understand and remember things.\n",
      "For example, when we see different views of the same object close\n",
      "together in time, our brain learns to recognize that object regardless\n",
      "of its position or orientation.  - USL: On the other hand, USL is\n",
      "about how our brains pick up on patterns and features in what we see\n",
      "without being told what to look for. It helps us develop a sense of\n",
      "what's important in our visual environment, like recognizing shapes or\n",
      "textures, even if we're not consciously trying to learn them.  Both\n",
      "UTL and USL are essential for our brains to adapt and understand the\n",
      "world around us, forming the basis for how we perceive and interpret\n",
      "visual information.\n"
     ]
    }
   ],
   "source": [
    "# second conversational cycle\n",
    "second_question = \"Explain the concepts of UTL and USL in layman's terms.\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_2[\"answer\"]])\n",
    "print_response(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised Temporal Learning (UTL) is a learning mechanism that\n",
      "exploits the temporal continuity of visual experiences to factor out\n",
      "feature identity from other visual attributes.\n"
     ]
    }
   ],
   "source": [
    "# third conversational cycle\n",
    "third_question = \"Define UTL in one sentence\"\n",
    "ai_msg_3 = rag_chain.invoke({\"input\": third_question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_3[\"answer\"]])\n",
    "print_response(ai_msg_3[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- EXAMPLE OF CONTEXTUAL COMPRESSION RETRIEVER ---- \n",
    "# from langchain.retrievers import ContextualCompressionRetriever\n",
    "# from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "# compressor = LLMChainExtractor.from_llm(llm)\n",
    "# compression_retriever = ContextualCompressionRetriever(\n",
    "#     base_compressor=compressor,\n",
    "#     base_retriever=vectordb.as_retriever()\n",
    "# )\n",
    "# question = \"what did they say about matlab?\"\n",
    "# compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "# pretty_print_docs(compressed_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
