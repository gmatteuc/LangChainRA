{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary modules\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import textwrap\n",
    "import dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OpenAI API key from secret file\n",
    "dotenv.load_dotenv()\n",
    "# language model to use\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0) # could also use Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to print the response in wrapped format\n",
    "def print_response(response, width=70):\n",
    "    # wrap the response to fit within the specified width\n",
    "    wrapper = textwrap.TextWrapper(width=width) \n",
    "    # wrap the response\n",
    "    wrapped_string = wrapper.fill(response)\n",
    "    # print the wrapped response\n",
    "    print(wrapped_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Ollama as model\n",
    "llm = Ollama(model=\"llama3\")\n",
    "# test the model with a prompt\n",
    "llm_response = llm.invoke(\"Tell me a joke on neuroscientist and language models\")\n",
    "print_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some mock documents to test rag\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
    "        metadata={\"source\": \"fish-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
    "        metadata={\"source\": \"bird-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: with OllamaEmbeddings it doen't work -----\n",
    "# create a Chroma vesctor store instance with the Ollama embeddings\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents,\n",
    "#     embedding=OllamaEmbeddings(model=\"llama3\"),\n",
    "# )\n",
    "# -------> OPEN ISSUE IN OLLAMA EMBEDDINGS <-------\n",
    "\n",
    "# NB: with OpenAI embeddings it does -----\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "# to clear the database (needed if running multiple times to avoid accumulation)\n",
    "# vectorstore.delete_collection() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 6 is greater than number of elements in index 5, updating n_results = 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Goldfish are popular pets for beginners, requiring relatively simple care.', metadata={'source': 'fish-pets-doc'}),\n",
       "  0.2768577039241791),\n",
       " (Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'source': 'mammal-pets-doc'}),\n",
       "  0.39694297313690186),\n",
       " (Document(page_content='Dogs are great companions, known for their loyalty and friendliness.', metadata={'source': 'mammal-pets-doc'}),\n",
       "  0.40311747789382935),\n",
       " (Document(page_content='Rabbits are social animals that need plenty of space to hop around.', metadata={'source': 'mammal-pets-doc'}),\n",
       "  0.4430447816848755),\n",
       " (Document(page_content='Parrots are intelligent birds capable of mimicking human speech.', metadata={'source': 'bird-pets-doc'}),\n",
       "  0.47815245389938354)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the vectorstore with a query\n",
    "vectorstore.similarity_search_with_score(query='simple to care for', k = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a retriever to search for the most similar (k=1) document\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 1},\n",
    ")\n",
    "# define a message to be filled in by runnable passthrough question to ask a question using the provided context\n",
    "message = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "# create a chat prompt template from the message\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "# define the rag (retrieval augmented generation) chain with a runnable passthrough question\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it seems that taking care of goldfish\n",
      "with scales is quite easy. The document states that goldfish are\n",
      "\"popular pets for beginners\" and require \"relatively simple care\".\n",
      "This suggests that caring for goldfish with scales is a\n",
      "straightforward process that doesn't require specialized knowledge or\n",
      "skills.\n"
     ]
    }
   ],
   "source": [
    "# invoke the rag chain passing through a question\n",
    "response = rag_chain.invoke(\"tell how easy is to take care of pets with scales\")\n",
    "# print model response\n",
    "print_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
